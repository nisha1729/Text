{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "Copy of Project_2_(NLP_Text_Classification).ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nisha1729/Text/blob/master/Copy_of_Project_2_(NLP_Text_Classification).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsS-N2NvnaPR",
        "colab_type": "text"
      },
      "source": [
        "# Project 3: Text Classification in PyTorch\n",
        "\n",
        "## Instructions\n",
        "\n",
        "* All the tasks that you need to complete in this project are either coding tasks (mentioned inside the code cells of the notebook with `#TODO` notations) or theoretical questions that you need to answer by editing the markdown question cells.\n",
        "* **Please make sure you read the [Notes](#Important-Notes) section carefully before you start the project.**\n",
        "\n",
        "## Introduction\n",
        "This project deals with neural text classification using PyTorch. Text classification is the process of assigning tags or categories to text according to its content. It's one of the fundamental tasks in Natural Language Processing (NLP) with broad applications such as sentiment analysis, topic labeling, spam detection, and intent detection.\n",
        "\n",
        "Text classification algorithms are at the heart of a variety of software systems that process text data at scale. Email software uses text classification to determine whether incoming mail is sent to the inbox or filtered into the spam folder. Discussion forums use text classification to determine whether comments should be flagged as inappropriate.\n",
        "\n",
        "**_Example:_** A simple example of text classification would be Spam Classification. Consider the bunch of emails that you would receive in the your personal inbox if the email service provider did not have a spam filter algorithm. Because of the spam filter, spam emails get redirected to the Spam folder, while you receive only non-spam (\"_ham_\") emails in your inbox.\n",
        "\n",
        "![](http://blog.yhat.com/static/img/spam-filter.png)\n",
        "\n",
        "## Task\n",
        "Here, we want you to focus on a specific type of text classification task, \"Document Classification into Topics\". It can be addressed as classifying text data or even large documents into separate discrete topics/genres of interest.\n",
        "\n",
        "\n",
        "![](https://miro.medium.com/max/700/1*YWEqFeKKKzDiNWy5UfrTsg.png)\n",
        "\n",
        "In this project, you will be working on classifying given text data into discrete topics or genres. You are given a bunch of text data, each of which has a label attached. We ask you to learn why you think the contents of the documents have been given these labels based on their words. You need to create a neural classifier that is trained on this given information. Once you have a trained classifier, it should be able to predict the label for any new document or text data sample that is fed to it. The labels need not have any meaning to us, nor to you necessarily.\n",
        "\n",
        "## Data\n",
        "There are various datasets that we can use for this purpose. This tutorial shows how to use the text classification datasets in the PyTorch library ``torchtext``. There are different datasets in this library like `AG_NEWS`, `SogouNews`, `DBpedia`, and others. This project will deal with training a supervised learning algorithm for classification using one of these datasets. In task 1 of this project, we will work with the `AG_NEWS` dataset.\n",
        "\n",
        "## Load Data\n",
        "\n",
        "A bag of **ngrams** feature is applied to capture some partial information about the local word order. In practice, bi-grams or tri-grams are applied to provide more benefits as word groups than only one word.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "*\"I love Neural Networks\"*\n",
        "* **Bi-grams:** \"I love\", \"love Neural\", \"Neural Networks\"\n",
        "* **Tri-grams:** \"I love Neural\", \"love Neural Networks\"\n",
        "\n",
        "In the code below, we have loaded the `AG_NEWS` dataset from the ``torchtext.datasets.TextClassification`` package with bi-grams feature. The dataset supports the ngrams method. By setting ngrams to 2, the example text in the dataset will be a list of single words plus bi-grams string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6c_cqVMe4g3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install git+https://github.com/pytorch/text.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlkX6FDPnaPV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "36fb9854-9f11-4e15-8e75-221f8c83d0d0"
      },
      "source": [
        "\"\"\"\n",
        "Load the AG_NEWS dataset in bi-gram features format.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torchtext\n",
        "from torchtext.datasets import text_classification\n",
        "import os\n",
        "\n",
        "NGRAMS = 2\n",
        "\n",
        "if not os.path.isdir('./.data'):\n",
        "    os.mkdir('./.data')\n",
        "\n",
        "train_dataset, test_dataset = text_classification.DATASETS['AG_NEWS'](\n",
        "    root='./.data', ngrams=NGRAMS, vocab=None)\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ag_news_csv.tar.gz: 11.8MB [00:00, 130MB/s]\n",
            "120000lines [00:10, 11551.30lines/s]\n",
            "120000lines [00:20, 5786.93lines/s]\n",
            "7600lines [00:01, 6028.67lines/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8me6_8NWnaPb",
        "colab_type": "text"
      },
      "source": [
        "## Model\n",
        "\n",
        "Our first simple model is composed of an [`EmbeddingBag`](https://pytorch.org/docs/stable/nn.html?highlight=embeddingbag#torch.nn.EmbeddingBag) layer and a linear layer.\n",
        "\n",
        "``EmbeddingBag`` computes the mean value of a “bag” of embeddings. The text entries here have different lengths. ``EmbeddingBag`` requires no padding here since the text lengths are saved in offsets. Additionally, since ``EmbeddingBag`` accumulates the average across the embeddings on the fly, ``EmbeddingBag`` can enhance the performance and memory efficiency to process a sequence of tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6IKKsWPnaPc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Import the necessary libraries\n",
        "import torch.nn as nn\n",
        "\n",
        "# TODO: Create a class TextClassifier. Remember that this class will be your model.\n",
        "class TextClassifier(nn.Module):\n",
        "\n",
        "    # TODO: Define the __init__() method with proper parameters\n",
        "    # (vocabulary size, dimensions of the embeddings, number of classes)\n",
        "    def __init__(self, vocab_size, dim, num_class):\n",
        "        super(TextClassifier, self).__init__()\n",
        "\n",
        "        # TODO: define the embedding layer\n",
        "        self.layer1 = nn.Embedding(vocab_size, dim)\n",
        "        \n",
        "        # TODO: define the linear forward layer\n",
        "        self.layer2 = nn.Linear(dim, num_class)\n",
        "        \n",
        "        # TODO: Initialize weights\n",
        "        self.init_weights()\n",
        "\n",
        "    # TODO: Define a method to initialize weights.\n",
        "    def init_weights(self):\n",
        "\n",
        "        # The weights should be random in the range of -0.5 to 0.5.\n",
        "        # You can initialize bias values as zero.\n",
        "\n",
        "        self.layer1.weight.data.normal_(-0.5, 0.5)\n",
        "\n",
        "        self.layer2.weight.data.normal_(-0.5, 0.5)\n",
        "        self.layer2.bias.data.fill_(0.)\n",
        "    \n",
        "    # TODO: Define the forward function.\n",
        "    def forward(self, x):\n",
        "        # This should calculate the embeddings and return the linear layer\n",
        "        # with calculated embedding values.\n",
        "        out1 = self.layer1(x)\n",
        "        out2 = self.layer2(out1)\n",
        "        return out2\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JSPieaznaPh",
        "colab_type": "text"
      },
      "source": [
        "## Check your data before you proceed!\n",
        "\n",
        "Okay, so we know that we are using the `AG_NEWS` dataset in this project, but do you know what does the data contain? What is the format of the data? How many classes of data are there in this dataset? We do not know, yet. Let's find out!`\n",
        "\n",
        "\n",
        "## Question 1:\n",
        "Create a new cell in this notebook and try to analyze the dataset that we loaded for you before. Report the following:\n",
        "* Vocabulary size (VOCAB_SIZE)\n",
        "* Number of classes (NUM_CLASS)\n",
        "* Names of the classes\n",
        "\n",
        "\n",
        "## Answer 1:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BS2oE3_mzdJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "58195a57-185d-4698-f9b8-02be258b50fe"
      },
      "source": [
        "VOCAB_SIZE = len(train_dataset.get_vocab())\n",
        "NUM_CLASS = len(train_dataset.get_labels())\n",
        "print(VOCAB_SIZE)\n",
        "print(NUM_CLASS)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1308844\n",
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tlx95HtgnaPj",
        "colab_type": "text"
      },
      "source": [
        "## Create an instance for your model\n",
        "\n",
        "Great! You have successfully completed a basic analysis of the data that you are going to work with. The vocab size is equal to the length of vocab (including single word and ngrams). The number of classes is equal to the number of labels. Copy paste the code statements you used in your analysis to complete the code below. Also, using these parameters, create an instance `model` of your text classifier `TextClassifier`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYXM7y2AVDYQ",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hERyl4WRnaPk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Paramters and model instance creation.\n",
        "'''\n",
        "\n",
        "# TODO: Instantiate the Vocabulary size and the number of classes\n",
        "# from the training dataset that we loaded for you.\n",
        "\n",
        "VOCAB_SIZE = len(train_dataset.get_vocab())\n",
        "NUM_CLASS = len(train_dataset.get_labels())\n",
        "\n",
        "# Hint: Remember that these are PyTorch datasets. So, there should be \n",
        "# readily available functions that you can use to save time. ;)\n",
        "\n",
        "EMBED_DIM = 32 \n",
        "\n",
        "# TODO: Instantiate the model with the parameters you defined above. \n",
        "# Remember to allocate it to your 'device' variable.\n",
        "\n",
        "model = TextClassifier(VOCAB_SIZE, EMBED_DIM, NUM_CLASS).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOTvp_xLnaPr",
        "colab_type": "text"
      },
      "source": [
        "## Generate batch\n",
        "\n",
        "Since the text entries have different lengths, you need to create a custom function to generate data batches and offsets. This function should be passed to the ``collate_fn`` parameter in the ``DataLoader`` call of pyTorch which you will use to create the data later on. The input to ``collate_fn`` is a list of tensors with the size of batch_size, and the ``collate_fn`` function packs them into a mini-batch. Pay attention here and make sure that ``collate_fn`` is declared as a top level definition. This ensures that the function is available in each worker. This is the reason why you need to define this custom function first before you call DataLoader().\n",
        "\n",
        "The text entries in the original data batch input are packed into a list and concatenated as a single tensor as the input of ``EmbeddingBag``. The offsets is a tensor of delimiters to represent the beginning index of the individual sequence in the text tensor. Label is a tensor saving the labels of individual text entries.\n",
        "\n",
        "Finish the function definition below. The function should take batch as an input parameter. Each entry in the batch contains a pair of values of the text and the corresponding label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vK2I9Bz_naPt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Finish the function definition.\n",
        "\n",
        "def generate_batch(__________):\n",
        "    \n",
        "    label = torch.tensor([__________ for entry in __________])\n",
        "    text = [__________ for entry in __________]\n",
        "    offsets = [0] + [len(entry) for entry in __________]\n",
        "\n",
        "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "    text = torch.cat(text)\n",
        "    \n",
        "    return text, offsets, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24RmvGUcnaPx",
        "colab_type": "text"
      },
      "source": [
        "## Define the train function\n",
        "\n",
        "Here, you need to define a function which you will use later on in the project to train your model. This is very similar to the training steps that you have encountered before in previous coding assignment(s). The outline of the function is something like this -\n",
        "\n",
        "* load the data as batches\n",
        "* iterate over the batches\n",
        "* find the model output for a forward pass\n",
        "* calculate the loss\n",
        "* perform backpropagation on the loss (optimize it)\n",
        "* find the training accuracy\n",
        "\n",
        "In addition to this, you also need to find the total loss and total training accuracy values. Also, you need to return the average values of the total loss and total accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4wyZacXnaPy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(train_data):\n",
        "\n",
        "    # Initial values of training loss and training accuracy\n",
        "    \n",
        "    train_loss = 0\n",
        "    train_acc = 0\n",
        "\n",
        "    # TODO: Use the PyTorch DataLoader class to load the data \n",
        "    # into shuffled batches of appropriate sizes into the variable 'data'.\n",
        "    # Remember, this is the place where you need to generate batches.\n",
        "    \n",
        "    \n",
        "    \n",
        "    for i, (text, offsets, cls) in enumerate(data):\n",
        "        \n",
        "        # TODO: What do you need to do in order to perform backprop on the optimizer?\n",
        "        \n",
        "        \n",
        "        \n",
        "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
        "        \n",
        "        # TODO: Store the output of the model in variable 'output'\n",
        "        \n",
        "        \n",
        "        \n",
        "        # TODO: Define the 'loss' variable (with respect to 'output' and 'cls').\n",
        "        # Also calculate the total loss in variable 'train_loss'\n",
        "        \n",
        "        \n",
        "        \n",
        "        # TODO: Perform the backward propagation on 'loss' and \n",
        "        # optimize it through the 'optimizer' step\n",
        "        \n",
        "        \n",
        "        \n",
        "        # TODO: Calculate and store the total training accuracy\n",
        "        # in the variable 'total_acc'.\n",
        "        # Remember, you need to find the \n",
        "        \n",
        "        \n",
        "\n",
        "    # TODO: Adjust the learning rate here using the scheduler step\n",
        "    \n",
        "    \n",
        "\n",
        "    return __________, __________"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTrB40W5naP1",
        "colab_type": "text"
      },
      "source": [
        "## Define the test function\n",
        "\n",
        "Using the framework of the `train()` function in the previous cell, try to figure out the structure of the test function below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2_Cs8URnaP2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(test_data):\n",
        "    \n",
        "    # Initial values of test loss and test accuracy\n",
        "    \n",
        "    loss = 0\n",
        "    acc = 0\n",
        "    \n",
        "    # TODO: Use DataLoader class to load the data\n",
        "    # into non-shuffled batches of appropriate sizes.\n",
        "    # Remember, you need to generate batches here too.\n",
        "    \n",
        "    \n",
        "    \n",
        "    for text, offsets, cls in data:\n",
        "        \n",
        "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
        "        \n",
        "        # Hint: There is a 'hidden hint' here. Let's see if you can find it :)\n",
        "        \n",
        "        \n",
        "            \n",
        "            # TODO: Get the model output\n",
        "            \n",
        "            \n",
        "            \n",
        "            # TODO: Calculate and add the loss to find total 'loss'\n",
        "        \n",
        "            \n",
        "            \n",
        "            # TODO: Calculate the accuracy and store it in the 'acc' variable\n",
        "            \n",
        "            \n",
        "\n",
        "    return ____________, ____________"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykQ7dEJBnaP6",
        "colab_type": "text"
      },
      "source": [
        "## Split the dataset and run the model\n",
        "\n",
        "The original `AG_NEWS` has no validation dataset. For this reason, you need to split the training dataset into training and validation sets with a proper split ratio. The `random_split()` function in the torch.utils core PyTorch library should be able to help you with this. We have already imported it for you. :)\n",
        "\n",
        "* Consider the initial learning rate as 4.0, number of epochs as 5, training data ratio as 0.9.\n",
        "* You need to define and use a proper loss function\n",
        "* Define an Optimization algorithm (Suggestion: SGD)\n",
        "* Define a scheduler function to adjust the learning rate through epochs (gamma parameter = 0.9).\n",
        "(Hint: Look at the `StepLR` function)\n",
        "* Monitor the loss and accuracy values for both training and validation data sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIB13x5pnaP7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "from torch.utils.data.dataset import random_split\n",
        "\n",
        "# TODO: Set the number of epochs and the learning rate to \n",
        "# their initial values here\n",
        "\n",
        "N_EPOCHS = __________\n",
        "LEARNING_RATE = __________\n",
        "TRAIN_RATIO = __________\n",
        "\n",
        "# TODO: Set the intial validation loss to positive infinity\n",
        "\n",
        "\n",
        "\n",
        "# TODO: Use the appropriate loss function\n",
        "\n",
        "\n",
        "\n",
        "# TODO: Use the appropriate optimization algorithm with parameters (Suggested: SGD)\n",
        "\n",
        "\n",
        "\n",
        "# TODO: Use a scheduler function\n",
        "\n",
        "\n",
        "\n",
        "# TODO: Split the data into train and validation sets using random_split()\n",
        "\n",
        "\n",
        "\n",
        "# TODO: Finish the rest of the code below\n",
        "\n",
        "for epoch in range(__________):\n",
        "\n",
        "    start_time = time.time()\n",
        "    train_loss, train_acc = train(__________)\n",
        "    valid_loss, valid_acc = test(__________)\n",
        "\n",
        "    secs = int(time.time() - start_time)\n",
        "    mins = secs / 60\n",
        "    secs = secs % 60\n",
        "\n",
        "    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
        "    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
        "    print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ugku2YwLnaP-",
        "colab_type": "text"
      },
      "source": [
        "## Let's  check the test loss and test accuracy\n",
        "\n",
        "So you have trained your model and seen how well it performs on the training and validation datasets. Now, you need to check your model's performance against the test dataset. Using the test dataset as input, report the test loss and test accuracy scores of your model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lihUDKMinaP_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Compete the code below to find \n",
        "# the results (loss and accuracy) on the test data\n",
        "\n",
        "print('Checking the results of test dataset...')\n",
        "test_loss, test_acc = __________\n",
        "print(f'\\tLoss: {test_loss:.4f}(test)\\t|\\tAcc: {test_acc * 100:.1f}%(test)')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rgpN26NnaQB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# importing necessary libraries\n",
        "\n",
        "import re\n",
        "from torchtext.data.utils import ngrams_iterator\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "# labels for the AG_NEWS dataset\n",
        "\n",
        "ag_news_label = {1 : \"World\",\n",
        "                 2 : \"Sports\",\n",
        "                 3 : \"Business\",\n",
        "                 4 : \"Sci/Tec\"}\n",
        "\n",
        "def predict(text, model, vocab, ngrams):\n",
        "    tokenizer = get_tokenizer(\"basic_english\")\n",
        "    with torch.no_grad():\n",
        "        text = torch.tensor([vocab[token]\n",
        "                            for token in ngrams_iterator(tokenizer(text), ngrams)])\n",
        "        output = model(text, torch.tensor([0]))\n",
        "        return output.argmax(1).item() + 1\n",
        "\n",
        "ex_text_str = \"MEMPHIS, Tenn. – Four days ago, Jon Rahm was \\\n",
        "    enduring the season’s worst weather conditions on Sunday at The \\\n",
        "    Open on his way to a closing 75 at Royal Portrush, which \\\n",
        "    considering the wind and the rain was a respectable showing. \\\n",
        "    Thursday’s first round at the WGC-FedEx St. Jude Invitational \\\n",
        "    was another story. With temperatures in the mid-80s and hardly any \\\n",
        "    wind, the Spaniard was 13 strokes better in a flawless round. \\\n",
        "    Thanks to his best putting performance on the PGA Tour, Rahm \\\n",
        "    finished with an 8-under 62 for a three-stroke lead, which \\\n",
        "    was even more impressive considering he’d never played the \\\n",
        "    front nine at TPC Southwind.\"\n",
        "\n",
        "vocab = train_dataset.get_vocab()\n",
        "model = model.to(\"cpu\")\n",
        "\n",
        "# TODO: Predict the topic of the above given random text (use bigrams)\n",
        "# Use the proper paramters in the predict() function\n",
        "\n",
        "print(\"This is a '%s' news\" % ag_news_label[predict(__________, __________, __________, __________)])\n",
        "\n",
        "# If you have done everything correctly in this task,\n",
        "# then the output of this cell should be - \"This is a 'Sports' news\"."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-7AekGPnaQE",
        "colab_type": "text"
      },
      "source": [
        "# Congratulations! You just designed your first neural classifier!\n",
        "\n",
        "And probably you have achieved a good accuracy score too. Great job!\n",
        "\n",
        "## Question 2:\n",
        "You just tested your model with a new sample text. Try to feed some more random examples of similar text (which you think are related to at least one of the four topics _\"World\", \"Sports\", \"Business\", \"Sci/Tec\"_ of our problem) to the model and see how your model reacts. Give at least 3 such examples (You are free to include more examples if you wish to).\n",
        "\n",
        "## Answer 2:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpfAifF4naQE",
        "colab_type": "text"
      },
      "source": [
        "## Question 3:\n",
        "Okay, probably the model still works great with the examples you fed to it in the previous question. How about a twist in the plot? Let's feed it some more random text data from completely different genres/topics (not belonging to the 4 topics which we talk about the in the first question). How does your model react now? Give at least 3 such examples (You are free to include more examples if you wish to).\n",
        "\n",
        "Of course the predictions will be limited to the four class labels that your model is trained on. Can you somehow justify the labels that your model predicted now for the given text inputs?\n",
        "\n",
        "## Answer 3:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwV1TptRnaQF",
        "colab_type": "text"
      },
      "source": [
        "## Question 4:\n",
        "Your model probably has achieved a good accuracy score. However, there may be lots of things that you could still try to do to improve your classifier model. Can you try to list down some improvements that you think would be able to improve the above model's performance?\n",
        "\n",
        "_(Hint: Maybe think about alternate architectures, #layers, hyper-paramters, etc..., but try not to come up with too complex stuff! :) )_\n",
        "\n",
        "## Answer 4:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndDsTlLlnaQG",
        "colab_type": "text"
      },
      "source": [
        "# Task 2: Try the better option that you proposed\n",
        "\n",
        "In Question 4, you have proposed some alternate solution that you think will be able to somehow improve your model. Following one of the options below, try to build and train a new model, and report the new loss and accuracy scores. Is it better than your initial classifier model for the same data?\n",
        "\n",
        "For your reference, here are some neural models using which researchers have tried to classify text before:\n",
        "\n",
        "* Recurrent Neural Networks (RNNs)\n",
        "* Long-Short Term Memory (LSTM)\n",
        "* Bi-directional LSTM (BiLSTM)\n",
        "* Gated Recurrent Units (GRUs)\n",
        "\n",
        "# Task 3: Let your creativity flow!\n",
        "\n",
        "As discussed earlier, you are free to come up with anything in task 3. Think and try to model unique (not too complex!) neural architecture on your own. Remember that this model has to be novel as much as possible, so try not to copy other people's existing work. Using the same data, train the new model, and report the accuracy scores. How much better/worse is this model than the previous two models? Why do you think this is better/worse?\n",
        "\n",
        "# Important Notes\n",
        "\n",
        "## NOTE 1:\n",
        "If you want, you can try out the models on other datasets too for comparisons. Although this is not mandatory, it would be really interesting to see how your model performs for data from different domains maybe. Note that you may need to tweak the code a little bit when you are considering other datasets and formats. \n",
        "\n",
        "## NOTE 2:\n",
        "Any form of plagiarism is strictly prohibited. If it is found that you have copied sample code from the internet, the entire team will be penalized.\n",
        "\n",
        "## NOTE 3:\n",
        "Often Jupyter Notebooks tend to stop working or crash due to overload of memory (lot of variables, big neural models, memory-intensive training of models, etc...). Moreover, with more number of tasks, the number of variables that you will be using will surely incerase. Therefore, it is recommended that you use separate notebooks for each _Task_ in this project.\n",
        "\n",
        "## NOTE 4:\n",
        "You are expected to write well-documented code, that is, with proper comments wherever you think is needed. Make sure you write a comprehensive report for the entire project consisting of data analysis, your model architecture, methods used, discussing and comparing the models against the accuracy and loss metrics, and a final conslusion. If you want to prepare separate reports for each _Task_, you could do this in the Jupyter Notebook itself using $Mardown$ and $\\LaTeX$ code if needed. If you want to submit a single report for the entire project, you could submit a PDF file in that case (Word or $\\LaTeX$).\n",
        "\n",
        "All the very best for project 2. Wishing you happy holidays and a very happy new year in advance! :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKNVAq1EnaQH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}